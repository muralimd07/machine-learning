{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated valued:\n",
      "449980.0\n"
     ]
    }
   ],
   "source": [
    "def estimate_home_value(size_in_sqft, number_of_bedrooms):\n",
    "\n",
    "    # Assume all homes are worth at least $50,000\n",
    "    value = 50000\n",
    "\n",
    "    # Adjust the value estimate based on the size of the house\n",
    "    value = value + (size_in_sqft * 92.1)\n",
    "\n",
    "    # Adjust the value estimate based on the number of bedrooms\n",
    "    value = value + (number_of_bedrooms * 10000)\n",
    "\n",
    "    return value\n",
    "\n",
    "# Estimate the value of our house:\n",
    "# - 5 bedrooms\n",
    "# - 3800 sq ft\n",
    "# Actual value: $450,000\n",
    "\n",
    "value = estimate_home_value(3800, 5)\n",
    "\n",
    "print(\"Estimated valued:\")\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import webbrowser\n",
    "import os\n",
    "\n",
    "# Read the dataset into a data table using Pandas\n",
    "data_table = pandas.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Create a web page view of the data for easy viewing\n",
    "html = data_table[0:100].to_html()\n",
    "\n",
    "# Save the html to a temporary file\n",
    "with open(\"data.html\", \"w\") as f:\n",
    "    f.write(html)\n",
    "\n",
    "# Open the web page in our web browser\n",
    "full_filename = os.path.abspath(\"data.html\")\n",
    "webbrowser.open(\"file://{}\".format(full_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1978, 1, 4, ..., 0, 0, 0],\n",
       "        [1958, 1, 3, ..., 0, 0, 0],\n",
       "        [2002, 1, 3, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1983, 1, 1, ..., 0, 0, 0],\n",
       "        [1981, 1, 3, ..., 0, 0, 0],\n",
       "        [1980, 1, 3, ..., 0, 0, 0]], dtype=object),\n",
       " array([ 270897.,  302404., 2519996., ...,   98280.,   98278.,  186480.]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Varies with sklearn version\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "\n",
    "# Remove the sale price from the feature data\n",
    "del features_df['sale_price']\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "features_df.to_csv('SampleX.csv')\n",
    "df['sale_price'].to_csv('SampleY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1988, 1, 3, ..., 0, 0, 0],\n",
       "        [2006, 2, 4, ..., 0, 0, 0],\n",
       "        [2004, 2, 4, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1974, 2, 5, ..., 0, 0, 0],\n",
       "        [2016, 2, 4, ..., 0, 0, 0],\n",
       "        [2015, 1, 2, ..., 0, 0, 0]], dtype=object),\n",
       " array([[2006, 1, 3, ..., 0, 0, 0],\n",
       "        [1974, 1, 3, ..., 0, 0, 0],\n",
       "        [2007, 2, 2, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [2007, 1, 4, ..., 0, 0, 0],\n",
       "        [1963, 1, 2, ..., 0, 0, 0],\n",
       "        [1987, 1, 4, ..., 0, 0, 0]], dtype=object),\n",
       " array([332013., 428396., 623702., ..., 573297., 655196., 373593.]),\n",
       " array([364141., 447298., 245702., ...,  94496., 157502., 433442.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "X_train, X_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='huber', max_depth=6,\n",
       "                          max_features=0.1, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=9, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                          n_iter_no_change=None, presort='auto', random_state=0,\n",
       "                          subsample=1.0, tol=0.0001, validation_fraction=0.1,\n",
       "                          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit regression model\n",
    "model = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=0.1,\n",
    "    loss='huber',\n",
    "    random_state=0\n",
    ")\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/trained_house_classifier_model.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model to a file so we can use it in other programs\n",
    "joblib.dump(model, 'model/trained_house_classifier_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 47577.9307\n",
      "Test Set Mean Absolute Error: 59826.0662\n"
     ]
    }
   ],
   "source": [
    "# Find the error rate on the training set\n",
    "mae = mean_absolute_error(y_train, model.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mae)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mae = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "city_New Michele - 0.00%\n",
      "city_Julieberg - 0.00%\n",
      "city_New Robinton - 0.00%\n",
      "city_Martinezfort - 0.00%\n",
      "city_Lake Jennifer - 0.00%\n",
      "city_Davidtown - 0.00%\n",
      "city_Rickytown - 0.01%\n",
      "city_South Stevenfurt - 0.01%\n",
      "city_East Justin - 0.01%\n",
      "city_West Brittanyview - 0.01%\n",
      "city_West Terrence - 0.01%\n",
      "city_Fosterberg - 0.01%\n",
      "city_Leahview - 0.02%\n",
      "city_Toddshire - 0.03%\n",
      "city_Wendybury - 0.03%\n",
      "city_Brownport - 0.03%\n",
      "city_Port Adamtown - 0.04%\n",
      "city_East Janiceville - 0.04%\n",
      "city_Joshuafurt - 0.04%\n",
      "city_Amystad - 0.05%\n",
      "city_Port Daniel - 0.06%\n",
      "city_Clarkberg - 0.08%\n",
      "city_Port Jonathanborough - 0.12%\n",
      "garage_type_detached - 0.12%\n",
      "city_Jenniferberg - 0.16%\n",
      "city_West Lydia - 0.18%\n",
      "city_Davidfort - 0.18%\n",
      "city_North Erinville - 0.23%\n",
      "city_Lewishaven - 0.25%\n",
      "city_East Amychester - 0.25%\n",
      "city_East Lucas - 0.27%\n",
      "city_Morrisport - 0.29%\n",
      "city_West Gerald - 0.30%\n",
      "city_West Gregoryview - 0.32%\n",
      "city_Lake Dariusborough - 0.34%\n",
      "city_Lake Carolyn - 0.35%\n",
      "has_central_heating - 0.38%\n",
      "city_Richardport - 0.38%\n",
      "city_West Ann - 0.46%\n",
      "city_South Anthony - 0.49%\n",
      "city_Justinport - 0.59%\n",
      "has_central_cooling - 0.66%\n",
      "half_bathrooms - 0.83%\n",
      "city_Hallfort - 0.83%\n",
      "garage_type_attached - 0.97%\n",
      "city_Chadstad - 1.02%\n",
      "city_Scottberg - 1.31%\n",
      "city_Port Andrealand - 1.40%\n",
      "city_Lake Christinaport - 1.41%\n",
      "stories - 1.63%\n",
      "city_Lake Jack - 1.80%\n",
      "has_fireplace - 2.42%\n",
      "carport_sqft - 3.22%\n",
      "city_Jeffreyhaven - 3.37%\n",
      "city_Coletown - 4.07%\n",
      "year_built - 4.25%\n",
      "garage_type_none - 5.26%\n",
      "full_bathrooms - 5.41%\n",
      "has_pool - 5.77%\n",
      "num_bedrooms - 6.08%\n",
      "total_sqft - 11.38%\n",
      "garage_sqft - 12.87%\n",
      "livable_sqft - 17.88%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# These are the feature labels from our data set\n",
    "feature_labels = np.array(['year_built', 'stories', 'num_bedrooms', 'full_bathrooms', 'half_bathrooms', 'livable_sqft', 'total_sqft', 'garage_sqft', 'carport_sqft', 'has_fireplace', 'has_pool', 'has_central_heating', 'has_central_cooling', 'garage_type_attached', 'garage_type_detached', 'garage_type_none', 'city_Amystad', 'city_Brownport', 'city_Chadstad', 'city_Clarkberg', 'city_Coletown', 'city_Davidfort', 'city_Davidtown', 'city_East Amychester', 'city_East Janiceville', 'city_East Justin', 'city_East Lucas', 'city_Fosterberg', 'city_Hallfort', 'city_Jeffreyhaven', 'city_Jenniferberg', 'city_Joshuafurt', 'city_Julieberg', 'city_Justinport', 'city_Lake Carolyn', 'city_Lake Christinaport', 'city_Lake Dariusborough', 'city_Lake Jack', 'city_Lake Jennifer', 'city_Leahview', 'city_Lewishaven', 'city_Martinezfort', 'city_Morrisport', 'city_New Michele', 'city_New Robinton', 'city_North Erinville', 'city_Port Adamtown', 'city_Port Andrealand', 'city_Port Daniel', 'city_Port Jonathanborough', 'city_Richardport', 'city_Rickytown', 'city_Scottberg', 'city_South Anthony', 'city_South Stevenfurt', 'city_Toddshire', 'city_Wendybury', 'city_West Ann', 'city_West Brittanyview', 'city_West Gerald', 'city_West Gregoryview', 'city_West Lydia', 'city_West Terrence'])\n",
    "\n",
    "# Load the trained model created with train_model.py\n",
    "model = joblib.load('model/trained_house_classifier_model.pkl')\n",
    "\n",
    "# Create a numpy array based on the model's feature importances\n",
    "importance = model.feature_importances_\n",
    "\n",
    "# Sort the feature labels based on the feature importance rankings from the model\n",
    "feauture_indexes_by_importance = importance.argsort()\n",
    "\n",
    "# Print each feature label, from most important to least important (reverse order)\n",
    "for index in feauture_indexes_by_importance:\n",
    "    print(\"{} - {:.2f}%\".format(feature_labels[index], (importance[index] * 100.0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 48618.2759\n",
      "Test Set Mean Absolute Error: 58927.1341\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "\n",
    "# Remove the sale price from the feature data\n",
    "del features_df['sale_price']\n",
    "\n",
    "# Create the X and y arrays\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit regression model\n",
    "model = ensemble.GradientBoostingRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=6,\n",
    "    min_samples_leaf=9,\n",
    "    max_features=0.1,\n",
    "    loss='huber'\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model to a file so we can use it in other programs\n",
    "joblib.dump(model, 'model/trained_house_classifier_model.pkl')\n",
    "\n",
    "# Find the error rate on the training set\n",
    "mse = mean_absolute_error(y_train, model.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mse = mean_absolute_error(y_test, model.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-313cd8b94ef9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDecisionTreeRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaBoostRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Load the data set\n",
    "df = pd.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pd.get_dummies(df, columns=['garage_type', 'city'])\n",
    "del features_df['sale_price']\n",
    "\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (DT) : 111283.3403\n",
      "Mean Absolute Error (AdaBoost) : 59571.7507\n"
     ]
    }
   ],
   "source": [
    "# Fit regression model\n",
    "regr_1 = DecisionTreeRegressor(max_depth=4)\n",
    "\n",
    "regr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=50),n_estimators=70)\n",
    "\n",
    "regr_1.fit(X_train, y_train)\n",
    "regr_2.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_1 = regr_1.predict(X_test)\n",
    "y_2 = regr_2.predict(X_test)\n",
    "\n",
    "mse = mean_absolute_error(y_test, y_1)\n",
    "print(\"Mean Absolute Error (DT) : %.4f\" % mse)\n",
    "\n",
    "mse = mean_absolute_error(y_test, y_2)\n",
    "print(\"Mean Absolute Error (AdaBoost) : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 96230.1727\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 7819301843446930.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reg = linear_model.SGDRegressor()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Mean Absolute Error: 7769785249299518.0000\n",
      "Test Set Mean Absolute Error: 7819301843446930.0000\n"
     ]
    }
   ],
   "source": [
    "#Let's find if it underfits...\n",
    "# Find the error rate on the training set\n",
    "mse = mean_absolute_error(y_train, reg.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set\n",
    "mse = mean_absolute_error(y_test, reg.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 64752.4880\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "reg = RandomForestRegressor()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 96231.9639\n"
     ]
    }
   ],
   "source": [
    "## Naive Bayes\n",
    "\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "reg = BayesianRidge()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error : 171395.1292\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "reg = SVR()\n",
    "reg.fit (X_train,y_train)\n",
    "pred=reg.predict(X_test)\n",
    "mse = mean_absolute_error(y_test,pred)\n",
    "print(\"Mean Absolute Error : %.4f\" % mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "from sklearn import ensemble\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Load the data set\n",
    "df = pandas.read_csv(\"ml_house_data_set.csv\")\n",
    "\n",
    "# Remove the fields from the data set that we don't want to include in our model\n",
    "del df['house_number']\n",
    "del df['unit_number']\n",
    "del df['street_name']\n",
    "del df['zip_code']\n",
    "\n",
    "# Replace categorical data with one-hot encoded data\n",
    "features_df = pandas.get_dummies(df, columns=['garage_type', 'city'])\n",
    "del features_df['sale_price']\n",
    "\n",
    "X = features_df.as_matrix()\n",
    "y = df['sale_price'].as_matrix()\n",
    "\n",
    "# Split the data set in a training set (70%) and a test set (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Create the model\n",
    "model = ensemble.GradientBoostingRegressor()\n",
    "\n",
    "# Parameters we want to try\n",
    "param_grid = {\n",
    "    'n_estimators': [500, 3000],\n",
    "    'max_depth': [4, 6],\n",
    "    'min_samples_leaf': [3, 9],\n",
    "    'learning_rate': [0.1, 0.05],\n",
    "    'max_features': [0.3, 0.1],\n",
    "    'loss': ['ls', 'huber']\n",
    "}\n",
    "\n",
    "#param_grid = {\n",
    "#    'n_estimators': [500, 1000, 3000],\n",
    "#    'max_depth': [4, 6],\n",
    "#   'min_samples_leaf': [3, 5, 9, 17],\n",
    "#    'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
    "#    'max_features': [1.0, 0.3, 0.1],\n",
    "#    'loss': ['ls', 'lad', 'huber']\n",
    "#}\n",
    "\n",
    "# Define the grid search we want to run. Run it with four cpus in parallel. -1 all cores\n",
    "gs_cv = GridSearchCV(model, param_grid, n_jobs=-1)\n",
    "\n",
    "# Run the grid search - on only the training data!\n",
    "gs_cv.fit(X_train, y_train)\n",
    "\n",
    "# Print the parameters that gave us the best result!\n",
    "print(gs_cv.best_params_)\n",
    "\n",
    "# After running a .....long..... time, the output will be something like\n",
    "# {'loss': 'huber', 'learning_rate': 0.1, 'min_samples_leaf': 9, 'n_estimators': 3000, 'max_features': 0.1, 'max_depth': 6}\n",
    "\n",
    "# That is the combination that worked best.\n",
    "\n",
    "# Find the error rate on the training set using the best parameters\n",
    "mse = mean_absolute_error(y_train, gs_cv.predict(X_train))\n",
    "print(\"Training Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n",
    "# Find the error rate on the test set using the best parameters\n",
    "mse = mean_absolute_error(y_test, gs_cv.predict(X_test))\n",
    "print(\"Test Set Mean Absolute Error: %.4f\" % mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Datasets\n",
    "Auto MPG : https://archive.ics.uci.edu/ml/datasets/auto+mpg\n",
    "    \n",
    "Auto MPG CSV format : https://www.kaggle.com/uciml/autompg-dataset    \n",
    "        \n",
    "Bike Sharing : https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset\n",
    "        \n",
    "Actual Feed: https://www.motivateco.com/use-our-data/\n",
    "        \n",
    "CSV Format: https://www.kaggle.com/marklvl/bike-sharing-dataset"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
